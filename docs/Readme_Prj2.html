<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
    "http://www.w3.org/TR/html4/loose.dtd">
<html><head>
<title>Web Database Classification </title>
<meta http-equiv="Content-type" content="text/html; charset=iso-8859-1">
</head>
<body bgcolor="#FFFFFF">

<p>
------------------------------------------<br>
Readme files for CS6111 Project 2<br>
Group members:<br>
Anuj Arora (aa2583)<br>
Jiacheng Yang (jy2522)<br>
------------------------------------------<br>
<br>


<b>1. List of files<br></b>
</p>
   * source code<br>

     - webdb/src/Test.java: the startup & ui module<br>

     - webdb/src/cache/*: module for creating cache<br>

     - webdb/src/classifier/*: module for classifying the database<br>

     - webdb/src/hierarchy/*: modules for building and maintaining the category hierarchy<br>

     - webdb/src/searcher/*: modules for querying the host<br>

     - webdb/src/util/*: utilities such as logger<br>

     - webdb/src/database/*: module for fetching and maintaining the resultset, either from the web or cache<br>

     - webdb/src/query/*:  Data classes to store result of query results<br>

     - webdb/src/summarizer/*: to create the database content summary<br>

     - webdb/src/lib/*: third-party libraries for common tasks such as encoding the search query<br>

     - webdb/src/rules.txt: rules for query probing<br>

     -webdb/src/hierarchy.xml: the category hierarchy in XML format

<br>


   * documents<br>
     - webdb/docs/Readme_Proj2.html: a readme file to decribe our methods<br>
     
     - webdb/src/Makefile: the makefile file <br>

     - webdb/docs/key: our Bing Api key
<br>
<br>
<b>2. How to run?</b>
<p>

   * Compile on clic machines <br>
     1) Go into directory webdb/src/ <br>
     2) option1: type 'make' <br>
     3) option2: manually compile: <br>
     	javac -cp ".:lib/*" Test.java <br>

</p>

<p>

   * Run tests <br>
     1) Go into directory webdb/src <br>
     2) option 1: run script ./run.sh apikey minspecificity mincoverage host<br>
     3) option 2: type make run ARG1=apikey ARG2=minspecificity ARG3=mincoverage ARG4=host <br>
     4) option 3: manually run java <br>
     	java -cp ".:lib/*" Test apikey minspecificity mincoverage host <br>
        
	Example: java -cp ".:lib/*" test your-bing-api
	0.6 100 yahoo.com <br>

     5) Resulting categories are displayed on screen <br>
     6) Content summary are captured in text files.  The file names are displayed on the screen<br>
<br>
</p>
<p>

<b>3. System design</b></br>
<br>
 Our system is decoupled into several modules. The main classTest.java in the default package processes the arguments passed to the and interacts with other modules to determine the category of the host, and generate the content summary.  
<br><br>
Following arguments are passed to the main function - <br>

1) API Key <br>
2) min-specificity <br> 
3) min-coverage <br>
4) host <br>
<br>
The program is broken into following modules - <br>
1) cache - The classes contained in this module provides caching of the data.  We cache following data - query probe results and sample documents that are collected for each query.  We provide an abstract class for caching, that 'DocumentCache' and 'ProbeCache' classes inherit from.  By providing the abstract class, we can easily add new classes to cache more data by inheriting from the abstract 'cache' class. <br>
<br>
2) Classifier - This module only contains DatabaseClassifier class.  It's the class that is used to by 'main' method in Test.java to classify the host.  <br>
<br>
3) Database - This module contains classes that are responsible for probing the host database.  Cached data is checked before requesting the data from the host.  If the data can be retrieved from the cache, host is not queried.  If the cache does not contain the data, host is queried and the data is added to cache.  We provide a abstract class here so that we can easily change/add various different probing algorithms without impacting the overall design.<br>
<br>
4) Hierarchy - This module is used to populate the cateogory hierarchy as provided in the project description.  It also contains classes to read and populate the rules associated with each category. <br>
<br>
5) Query - This module contains the data classes and the parser class to populate the data structures for the results retrieved by probing the host. <br>
<br>
6) Searcher - This module contains the classes to populate and execute bing query based on on the query terms associated with a category.  We provide an abstract class here, to further expand it to a different search engine. <br>
<br>
7) Summarizer - This module contains the classes to download the webpages and generate the content summary.  Lynx is used to download the webpage as described in the project description. <br>
<br>
8) util - this module contains a singleton Logger class to create the run.log and error.log.<br>
<br>
<br>
<b>3.1. Data Classification</b><br> 
The host database is classified based on the algorithm mentioned in the project description.  The category tree is built based on the categories provided in the project. We read the category hierarchy from a <b>XML</b> file.  As the classifier analyzes each category for the host database, it adds top 4 document URLs associated with each rule for the category under inspection.  These URLs are used to create the content summary classification in section 3.2.<br>  
<br>
All the data that is retrieved by the querying the host database is cached for reruns.  This includes probing data and documents associated with the category and sample data respectively.  Queries are logged in 'run.log' as the host database is queried. <br>
<br>
<b>3.2. Creating Document Summary</b><br>
The document summary is created based on the algorithm described in the project.  Before initiating the ContentSummarizer, the samples for the parents of the categories under which the host is classified, are aggregated.  Example - if the host is classified under 'Health' and "sports', the 'Root' samples will contain a samples collected for 'Root' plus samples collected for 'Health' plus samples collection for 'Sports'.  We use 'Sets' and 'Maps' to stored the samples as needed.  This helps in eliminating any duplicate values.  <br>
<br>
Once we have the final set of samples for all the categories (including parent categories) for which we need to generate content summary, we use the ContentSummarizer object to create content summary for each category.  The ContentSummarizer uses the HtmlParser to download and parse each sample URL.  The HtmlParser is based on Lynx and uses the code that is provided with the project description.  The document frequency is calculated by add each word as it is encountered in a document in a TreeMap.  The key of the TreeMap is the word itself, and value associated with the key is the document frequency.  As each document is parsed, the value of the 'Word' in the TreeMap is incremented.<br>
<br>
It should be noed that the documents are cached by the HtmlParser after they are downloaded. 
<br>
<br>

<b>4. Appendix</b>
<p>
* Third-party libraries<br>

   We use following third party libraries:<br>
<br>
   List of third-party libraries and our usage:<br>

    common-codecs - to encode the query strings  <br>

</p>
</body>
</html>
